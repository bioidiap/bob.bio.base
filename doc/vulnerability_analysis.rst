.. author: Yannick Dayer <yannick.dayer@idiap.ch>
.. date: 2021-04-14 09:39:37 +02

..  _bob.bio.base.vulnerability_analysis:

======================
Vulnerability analysis
======================

When developing a biometric system, one could be interested in knowing how it
performs when under attack. This is where the vulnerability to presentation
attacks comes in. This analysis allows to test a variety of attacks and assess
the cases where a biometric system is fooled.


Evaluation
----------

The evaluation of a system is done in two steps:

  - The scores are computed by feeding a database to a system;
  - Metrics and plots are extracted from the resulting scores.

Computing the scores
^^^^^^^^^^^^^^^^^^^^

To evaluate a biometric system, a series of samples coming from a dataset are
compared to an enrolled model, and each comparison should result in a score.
Generally, the score should be high if the compared samples come from the same
individual represented in the model.

In the case of vulnerability analysis, presentation attack samples are also
compared to the enrolled model of the attacked individual and give a score.
Ideally, the system should not recognize the attack sample and give a low score
to those comparison. But if the attack is well crafted, these attack samples
will pass as genuine.

To compute the comparison scores of a dataset on a system, the
vanilla_biometrics pipeline can be used. Only the dataset must be specifically
build for vulnerability analysis.
For a face presentation attack analysis (using the `replay mobile` dataset) on a
face recognition system (Facenet from Sandberg, in this case), use the following
command:

.. code_block:: sh

  $ bob bio pipelines vanilla-biometrics -m -v -o results/vuln -g dev -g eval replaymobile-img facenet-sanderberg

The ``-m`` (``--write-metadata-scores``) option is necessary, as the attack
samples will need to retain the information of being an attack or not, that is
passed as metadata.


Analyzing the scores
^^^^^^^^^^^^^^^^^^^^

To evaluate the performance of the system, we can analyze the distribution of
scores generated by the previous command. This step includes a series of
commands that generate plots and tables of metrics useful to assess the
performance and vulnerability of the system, or to compare multiple systems'
performance.

Each command is in the form:

.. code_block:: sh

  $ bob vulnerability <command> <options> <score files>

or alternatively:

.. code_block:: sh

  $ bob vuln <command> <options> <score files>

For a list of available commands, run:

.. code_block:: sh

  $ bob vuln --help

For more information on a specific command (available options, number of score
files), you can use the integrated help option:

.. code_block:: sh

  $ bob vuln metrics --help


Metrics
-------

This command generates a list of useful metrics like the FMR, the FNMR, the
IAPMR, for a specific operating point (threshold value).

It is possible to specify a value for the threshold, or a criterion can be used
to compute this value automatically by minimizing an error rate.

This function is useful to get a quick evaluation of a system on a single
operating point.


